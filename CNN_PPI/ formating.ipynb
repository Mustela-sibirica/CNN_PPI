{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 727\n",
    "LENGTH = 512\n",
    "EPOCHES = 100\n",
    "BATCH_SIZE = 300\n",
    "HIDDEN_UNITS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi_file = r'.\\data\\HIPPIE_v2.1.tsv'\n",
    "fasta_file = r'.\\data\\HUMAN_uniport.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "names = []\n",
    "sequences = []\n",
    "for seq in records:\n",
    "    names.append(str(seq.name))\n",
    "    sequences.append(str(seq.seq))\n",
    "protein_dict = dict(zip(names,sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ppi_file, 'r') as ppi:\n",
    "    pro_1 = []\n",
    "    pro_2 = []\n",
    "    pro_1_seq = []\n",
    "    pro_2_seq = []\n",
    "    for line in ppi.readlines():\n",
    "        line_list = line.strip().split(\"\\t\") \n",
    "        if re.search(r'_HUMAN', line_list[0]) and re.search(r'_HUMAN', line_list[2]) and float(line_list[4]) >= 0.72 and line_list[0] in protein_dict and line_list[2] in protein_dict:\n",
    "            if len(protein_dict[line_list[0]]) <= LENGTH and len(protein_dict[line_list[2]]) <= LENGTH:\n",
    "                pro_1.append(line_list[0])\n",
    "                pro_2.append(line_list[2])\n",
    "                pro_1_seq.append(protein_dict[line_list[0]])\n",
    "                pro_2_seq.append(protein_dict[line_list[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机对照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "number_random_seqences = len(pro_1)\n",
    "random_pro_1 = []\n",
    "random_pro_2 = []\n",
    "random_pro_1_seq = []\n",
    "random_pro_2_seq = []\n",
    "for i in range(number_random_seqences):\n",
    "    pro_1_index = random.randint(0,len(names))\n",
    "    pro_2_index = random.randint(0,len(names))\n",
    "    random_pro_1.append(names[pro_1_index])\n",
    "    random_pro_2.append(names[pro_2_index])\n",
    "    random_pro_1_seq.append(protein_dict[names[pro_1_index]])\n",
    "    random_pro_2_seq.append(protein_dict[names[pro_2_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "squences_1 = []\n",
    "squences_2 = []\n",
    "random_squences_1 = []\n",
    "random_squences_2 = []\n",
    "for seq in pro_1_seq:\n",
    "    if len(seq) < LENGTH:\n",
    "        squences_1.append(\"\".join(list(seq) + (['0'] * (LENGTH - len(seq)))))\n",
    "    else:\n",
    "        squences_1.append(str(seq[0:LENGTH]))\n",
    "for seq in pro_2_seq:\n",
    "    if len(seq) < LENGTH:\n",
    "        squences_2.append(\"\".join(list(seq) + (['0'] * (LENGTH - len(seq)))))\n",
    "    else:\n",
    "        squences_2.append(str(seq[0:LENGTH]))\n",
    "for seq in random_pro_1_seq:\n",
    "    if len(seq) < LENGTH:\n",
    "        random_squences_1.append(\"\".join(list(seq) + (['0'] * (LENGTH - len(seq)))))\n",
    "    else:\n",
    "        random_squences_1.append(str(seq[0:LENGTH]))\n",
    "for seq in random_pro_2_seq:\n",
    "    if len(seq) < LENGTH:\n",
    "        random_squences_2.append(\"\".join(list(seq) + (['0'] * (LENGTH - len(seq)))))\n",
    "    else:\n",
    "        random_squences_2.append(str(seq[0:LENGTH]))\n",
    "pro_1_seq = squences_1\n",
    "pro_2_seq = squences_2\n",
    "random_pro_1_seq = random_squences_1\n",
    "random_pro_2_seq = random_squences_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dual = [''] * (len(pro_1) + len(random_pro_1))\n",
    "for i in range(len(pro_1)):\n",
    "    pro_dual[i] = str(pro_1_seq[i]) + '1' + str(pro_2_seq[i])\n",
    "for i in range(len(random_pro_1)):\n",
    "    pro_dual[len(pro_1) + i] = str(random_pro_1_seq[i]) + '1' + str(random_pro_2_seq[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 格式整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toOneHot(base):\n",
    "    if base == \"C\":\n",
    "        base_one_hot = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"S\":\n",
    "        base_one_hot = [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"T\":\n",
    "        base_one_hot = [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"P\":\n",
    "        base_one_hot = [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"A\":\n",
    "        base_one_hot = [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"G\":\n",
    "        base_one_hot = [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"N\":\n",
    "        base_one_hot = [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"D\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"E\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"Q\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"H\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"R\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]\n",
    "    elif base == \"K\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]\n",
    "    elif base == \"M\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
    "    elif base == \"I\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]\n",
    "    elif base == \"L\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
    "    elif base == \"V\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]\n",
    "    elif base == \"F\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]\n",
    "    elif base == \"Y\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
    "    elif base == \"W\":\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "    elif base == \"1\":\n",
    "        base_one_hot = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]     #分隔符   \n",
    "    else:\n",
    "        base_one_hot = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    return base_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "squence_list = []\n",
    "result_list = []\n",
    "for seq in pro_dual:\n",
    "    seq_list = list(seq)\n",
    "    for base in seq_list:\n",
    "        squence_list.append(toOneHot(base))\n",
    "    squence_array = np.array(squence_list)\n",
    "    result_list.append(np.transpose(squence_array))\n",
    "    squence_list = []\n",
    "X = np.array(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_list = [\"1\"] * len(pro_1) + [\"0\"] * len(random_pro_1)\n",
    "Y = np.array(Y_list)\n",
    "Y = np.transpose(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 垃圾回收"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(records, names, sequences, protein_dict, \n",
    "    pro_1, pro_2, pro_1_seq, pro_2_seq,\n",
    "    random_pro_1, random_pro_2, random_pro_1_seq, random_pro_2_seq,\n",
    "    squences_1, squences_2, random_squences_1, random_squences_2,\n",
    "    pro_dual, squence_list, result_list, seq_list, Y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data for train: 42621\n",
      "number of data for test: 10445\n",
      "number of label for train: 42621\n",
      "number of label for test: 10445\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "msk = np.random.rand(len(Y)) < 0.8\n",
    "X_train = X[msk]\n",
    "Y_train = Y[msk]\n",
    "X_test = X[~msk]\n",
    "Y_test = Y[~msk]\n",
    "print(\"number of data for train: \" + str(len(X_train)))\n",
    "print(\"number of data for test: \" + str(len(X_test)))\n",
    "print(\"number of label for train: \" + str(len(Y_train)))\n",
    "print(\"number of label for test: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 格式整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9dc7a456c831>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train4D\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLENGTH\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test4D\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLENGTH\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train4D=X_train.reshape(X_train.shape[0],LENGTH*2+1,20,1).astype('float32')\n",
    "X_test4D=X_test.reshape(X_test.shape[0],LENGTH*2+1,20,1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_One_Hot = np_utils.to_categorical(Y_train)\n",
    "Y_test_One_Hot = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 垃圾回收"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(X, Y, X_train, X_test, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/X_train4D.npy\",X_train4D)\n",
    "np.save(\"./data/X_test4D.npy\",X_test4D)\n",
    "np.save(\"./data/Y_train_One_Hot.npy\",Y_train_One_Hot)\n",
    "np.save(\"./data/Y_test_One_Hot.npy\",Y_test_One_Hot)\n",
    "np.save(\"./data/Y_test_One_Hot.npy\",Y_test_One_Hot)\n",
    "np.save(\"./data/Y_test.npy\",Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
